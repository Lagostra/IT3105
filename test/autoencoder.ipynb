{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tflowtools as tft\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 8\n",
    "hidden_nodes = 8\n",
    "output_size = input_size\n",
    "learning_rate = 0.01\n",
    "num_steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cases = np.array(tft.gen_all_one_hot_cases(input_size))\n",
    "inputs = all_cases[:,0]\n",
    "targets = all_cases[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt = tf.placeholder('float')\n",
    "x = inpt\n",
    "y = x\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([input_size, hidden_nodes]))\n",
    "w2 = tf.Variable(tf.random_normal([hidden_nodes, output_size]))\n",
    "\n",
    "b1 = tf.Variable(tf.random_normal([hidden_nodes]))\n",
    "b2 = tf.Variable(tf.random_normal([output_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.nn.relu(tf.add(tf.matmul(x, w1), b1))\n",
    "x = tf.nn.relu(tf.add(tf.matmul(x, w2), b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.pow(y - x, 2))\n",
    "#loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=x, labels=y)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 50] Loss: 0.18973907828330994\n",
      "[Step 100] Loss: 0.08546511083841324\n",
      "[Step 150] Loss: 0.07924722135066986\n",
      "[Step 200] Loss: 0.07838166505098343\n",
      "[Step 250] Loss: 0.07817509025335312\n",
      "[Step 300] Loss: 0.07813261449337006\n",
      "[Step 350] Loss: 0.07812588661909103\n",
      "[Step 400] Loss: 0.07812508195638657\n",
      "[Step 450] Loss: 0.0781250074505806\n",
      "[Step 500] Loss: 0.078125\n",
      "[Step 550] Loss: 0.078125\n",
      "[Step 600] Loss: 0.078125\n",
      "[Step 650] Loss: 0.078125\n",
      "[Step 700] Loss: 0.078125\n",
      "[Step 750] Loss: 0.078125\n",
      "[Step 800] Loss: 0.078125\n",
      "[Step 850] Loss: 0.078125\n",
      "[Step 900] Loss: 0.078125\n",
      "[Step 950] Loss: 0.078125\n",
      "[Step 1000] Loss: 0.078125\n",
      "[Step 1050] Loss: 0.078125\n",
      "[Step 1100] Loss: 0.078125\n",
      "[Step 1150] Loss: 0.078125\n",
      "[Step 1200] Loss: 0.078125\n",
      "[Step 1250] Loss: 0.078125\n",
      "[Step 1300] Loss: 0.078125\n",
      "[Step 1350] Loss: 0.078125\n",
      "[Step 1400] Loss: 0.078125\n",
      "[Step 1450] Loss: 0.078125\n",
      "[Step 1500] Loss: 0.078125\n",
      "[Step 1550] Loss: 0.078125\n",
      "[Step 1600] Loss: 0.078125\n",
      "[Step 1650] Loss: 0.078125\n",
      "[Step 1700] Loss: 0.078125\n",
      "[Step 1750] Loss: 0.078125\n",
      "[Step 1800] Loss: 0.078125\n",
      "[Step 1850] Loss: 0.078125\n",
      "[Step 1900] Loss: 0.078125\n",
      "[Step 1950] Loss: 0.078125\n",
      "[Step 2000] Loss: 0.078125\n",
      "[Step 2050] Loss: 0.078125\n",
      "[Step 2100] Loss: 0.078125\n",
      "[Step 2150] Loss: 0.078125\n",
      "[Step 2200] Loss: 0.078125\n",
      "[Step 2250] Loss: 0.078125\n",
      "[Step 2300] Loss: 0.078125\n",
      "[Step 2350] Loss: 0.078125\n",
      "[Step 2400] Loss: 0.078125\n",
      "[Step 2450] Loss: 0.078125\n",
      "[Step 2500] Loss: 0.078125\n",
      "[Step 2550] Loss: 0.078125\n",
      "[Step 2600] Loss: 0.078125\n",
      "[Step 2650] Loss: 0.078125\n",
      "[Step 2700] Loss: 0.078125\n",
      "[Step 2750] Loss: 0.078125\n",
      "[Step 2800] Loss: 0.078125\n",
      "[Step 2850] Loss: 0.078125\n",
      "[Step 2900] Loss: 0.078125\n",
      "[Step 2950] Loss: 0.078125\n",
      "[Step 3000] Loss: 0.078125\n",
      "[Step 3050] Loss: 0.078125\n",
      "[Step 3100] Loss: 0.078125\n",
      "[Step 3150] Loss: 0.078125\n",
      "[Step 3200] Loss: 0.078125\n",
      "[Step 3250] Loss: 0.078125\n",
      "[Step 3300] Loss: 0.078125\n",
      "[Step 3350] Loss: 0.078125\n",
      "[Step 3400] Loss: 0.078125\n",
      "[Step 3450] Loss: 0.078125\n",
      "[Step 3500] Loss: 0.078125\n",
      "[Step 3550] Loss: 0.078125\n",
      "[Step 3600] Loss: 0.078125\n",
      "[Step 3650] Loss: 0.078125\n",
      "[Step 3700] Loss: 0.078125\n",
      "[Step 3750] Loss: 0.078125\n",
      "[Step 3800] Loss: 0.078125\n",
      "[Step 3850] Loss: 0.078125\n",
      "[Step 3900] Loss: 0.078125\n",
      "[Step 3950] Loss: 0.078125\n",
      "[Step 4000] Loss: 0.078125\n",
      "[Step 4050] Loss: 0.078125\n",
      "[Step 4100] Loss: 0.078125\n",
      "[Step 4150] Loss: 0.078125\n",
      "[Step 4200] Loss: 0.078125\n",
      "[Step 4250] Loss: 0.078125\n",
      "[Step 4300] Loss: 0.078125\n",
      "[Step 4350] Loss: 0.078125\n",
      "[Step 4400] Loss: 0.078125\n",
      "[Step 4450] Loss: 0.078125\n",
      "[Step 4500] Loss: 0.078125\n",
      "[Step 4550] Loss: 0.078125\n",
      "[Step 4600] Loss: 0.078125\n",
      "[Step 4650] Loss: 0.078125\n",
      "[Step 4700] Loss: 0.078125\n",
      "[Step 4750] Loss: 0.078125\n",
      "[Step 4800] Loss: 0.078125\n",
      "[Step 4850] Loss: 0.078125\n",
      "[Step 4900] Loss: 0.078125\n",
      "[Step 4950] Loss: 0.078125\n",
      "[Step 5000] Loss: 0.078125\n",
      "[Step 5050] Loss: 0.078125\n",
      "[Step 5100] Loss: 0.078125\n",
      "[Step 5150] Loss: 0.078125\n",
      "[Step 5200] Loss: 0.078125\n",
      "[Step 5250] Loss: 0.078125\n",
      "[Step 5300] Loss: 0.078125\n",
      "[Step 5350] Loss: 0.078125\n",
      "[Step 5400] Loss: 0.078125\n",
      "[Step 5450] Loss: 0.078125\n",
      "[Step 5500] Loss: 0.078125\n",
      "[Step 5550] Loss: 0.078125\n",
      "[Step 5600] Loss: 0.078125\n",
      "[Step 5650] Loss: 0.078125\n",
      "[Step 5700] Loss: 0.078125\n",
      "[Step 5750] Loss: 0.078125\n",
      "[Step 5800] Loss: 0.078125\n",
      "[Step 5850] Loss: 0.078125\n",
      "[Step 5900] Loss: 0.078125\n",
      "[Step 5950] Loss: 0.078125\n",
      "[Step 6000] Loss: 0.078125\n",
      "[Step 6050] Loss: 0.078125\n",
      "[Step 6100] Loss: 0.078125\n",
      "[Step 6150] Loss: 0.078125\n",
      "[Step 6200] Loss: 0.078125\n",
      "[Step 6250] Loss: 0.078125\n",
      "[Step 6300] Loss: 0.078125\n",
      "[Step 6350] Loss: 0.078125\n",
      "[Step 6400] Loss: 0.078125\n",
      "[Step 6450] Loss: 0.078125\n",
      "[Step 6500] Loss: 0.078125\n",
      "[Step 6550] Loss: 0.078125\n",
      "[Step 6600] Loss: 0.078125\n",
      "[Step 6650] Loss: 0.078125\n",
      "[Step 6700] Loss: 0.078125\n",
      "[Step 6750] Loss: 0.078125\n",
      "[Step 6800] Loss: 0.078125\n",
      "[Step 6850] Loss: 0.078125\n",
      "[Step 6900] Loss: 0.078125\n",
      "[Step 6950] Loss: 0.078125\n",
      "[Step 7000] Loss: 0.078125\n",
      "[Step 7050] Loss: 0.078125\n",
      "[Step 7100] Loss: 0.078125\n",
      "[Step 7150] Loss: 0.078125\n",
      "[Step 7200] Loss: 0.078125\n",
      "[Step 7250] Loss: 0.078125\n",
      "[Step 7300] Loss: 0.078125\n",
      "[Step 7350] Loss: 0.078125\n",
      "[Step 7400] Loss: 0.078125\n",
      "[Step 7450] Loss: 0.078125\n",
      "[Step 7500] Loss: 0.078125\n",
      "[Step 7550] Loss: 0.078125\n",
      "[Step 7600] Loss: 0.078125\n",
      "[Step 7650] Loss: 0.078125\n",
      "[Step 7700] Loss: 0.078125\n",
      "[Step 7750] Loss: 0.078125\n",
      "[Step 7800] Loss: 0.078125\n",
      "[Step 7850] Loss: 0.078125\n",
      "[Step 7900] Loss: 0.078125\n",
      "[Step 7950] Loss: 0.078125\n",
      "[Step 8000] Loss: 0.078125\n",
      "[Step 8050] Loss: 0.078125\n",
      "[Step 8100] Loss: 0.078125\n",
      "[Step 8150] Loss: 0.078125\n",
      "[Step 8200] Loss: 0.078125\n",
      "[Step 8250] Loss: 0.078125\n",
      "[Step 8300] Loss: 0.078125\n",
      "[Step 8350] Loss: 0.078125\n",
      "[Step 8400] Loss: 0.078125\n",
      "[Step 8450] Loss: 0.078125\n",
      "[Step 8500] Loss: 0.078125\n",
      "[Step 8550] Loss: 0.078125\n",
      "[Step 8600] Loss: 0.078125\n",
      "[Step 8650] Loss: 0.078125\n",
      "[Step 8700] Loss: 0.078125\n",
      "[Step 8750] Loss: 0.078125\n",
      "[Step 8800] Loss: 0.078125\n",
      "[Step 8850] Loss: 0.078125\n",
      "[Step 8900] Loss: 0.078125\n",
      "[Step 8950] Loss: 0.078125\n",
      "[Step 9000] Loss: 0.078125\n",
      "[Step 9050] Loss: 0.078125\n",
      "[Step 9100] Loss: 0.078125\n",
      "[Step 9150] Loss: 0.078125\n",
      "[Step 9200] Loss: 0.078125\n",
      "[Step 9250] Loss: 0.078125\n",
      "[Step 9300] Loss: 0.078125\n",
      "[Step 9350] Loss: 0.078125\n",
      "[Step 9400] Loss: 0.078125\n",
      "[Step 9450] Loss: 0.078125\n",
      "[Step 9500] Loss: 0.078125\n",
      "[Step 9550] Loss: 0.078125\n",
      "[Step 9600] Loss: 0.078125\n",
      "[Step 9650] Loss: 0.078125\n",
      "[Step 9700] Loss: 0.078125\n",
      "[Step 9750] Loss: 0.078125\n",
      "[Step 9800] Loss: 0.078125\n",
      "[Step 9850] Loss: 0.078125\n",
      "[Step 9900] Loss: 0.078125\n",
      "[Step 9950] Loss: 0.078125\n",
      "[Step 10000] Loss: 0.078125\n",
      "[array([0, 0, 0, 0, 4, 5, 0, 7], dtype=int64)]\n",
      "[[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(1, num_steps + 1):\n",
    "        _, l = sess.run([train_op, loss], feed_dict={inpt: inputs})\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print('[Step {}] Loss: {}'.format(i, l))\n",
    "    \n",
    "    softmax = tf.nn.softmax(x)\n",
    "    argmax = tf.argmax(softmax)\n",
    "    one_hot = tf.one_hot(argmax, 8)\n",
    "    result = sess.run([argmax], feed_dict={inpt: inputs})\n",
    "    print(result)\n",
    "    print(inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow CPU (Python 3.6)",
   "language": "python",
   "name": "tensorflow-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
